# ğŸ”¥ XOR PROBLEM â€” Multi-Layer Neural Network (MLP)

XOR is a classic problem in neural networks:

| Input | Output |
| ----- | ------ |
| 0,0   | 0      |
| 0,1   | 1      |
| 1,0   | 1      |
| 1,1   | 0      |

- XOR is **not linearly separable** â†’ cannot be solved by a single-layer perceptron.
- Requires **multi-layer neural network** (MLP).

---

## 1ï¸âƒ£ Forward Pass (Make Predictions)

- Input data `X` is fed into the network.
- Input layer â†’ hidden layer (4 neurons) â†’ output layer (1 neuron).
- Hidden layer uses **ReLU** activation.
- Output layer uses **Sigmoid** activation.
- Output `Å·` (prediction) is produced.
- Internally, PyTorch builds a **computation graph**.

---

## 2ï¸âƒ£ Calculate Loss (How Wrong the Model Is)

- Compare predictions `Å·` with true labels `y`.
- Use **Binary Cross Entropy Loss** (BCELoss) because output is 0 or 1.
- Loss is a **single scalar** representing error.

---

## 3ï¸âƒ£ Backward Pass (Calculate Gradients)

- Call `loss.backward()`.
- PyTorch computes **gradients using backpropagation**.
- Computes âˆ‚Loss/âˆ‚Weight and âˆ‚Loss/âˆ‚Bias for all parameters.
- Stored in:
  - `weight.grad`
  - `bias.grad`

Gradients indicate:

> How much each weight contributes to the error.

---

## 4ï¸âƒ£ Update Weights (Optimizer Step)

- Optimizer uses gradients to update weights.
- We use **Adam optimizer**.
- Update rule conceptually:

- After update, model becomes slightly better.

---

## 5ï¸âƒ£ Repeat for Many Epochs

- One full pass through the training data = **1 epoch**.
- Repeat steps: forward â†’ loss â†’ backward â†’ optimizer step.
- Training continues until model learns XOR pattern.
- We use **5000 epochs** for convergence.

---

## 6ï¸âƒ£ Validate on Test Data

- Use same input data (or unseen data) to check predictions.
- Turn off gradient tracking: `torch.no_grad()`.
- Only run forward pass.
- Output should be close to `[0, 1, 1, 0]`.

---

## ğŸ” Complete Loop Summary

1. Reset gradients
2. Forward pass â†’ predictions
3. Compute loss
4. Backward pass â†’ compute gradients
5. Optimizer weight update
6. Repeat for many epochs
7. Validate / predict

This is the complete XOR MLP training process.
