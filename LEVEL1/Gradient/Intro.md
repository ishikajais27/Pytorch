#GRADIENT
Tells how much to change the weight and biases to improve your model, derivative of loss function wrt to weights.Slope of loss function.
If gradient is positive, decrease the weight → loss decreases.
If gradient is negative, increase the weight → loss decreases.

                                                New Weight = Old Weight−Learning Rate×Gradient
Learning rate(Hyperparameters) - controls how big step model should take while updating the weight during training.